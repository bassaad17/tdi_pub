{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "# Image Classification with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Image classification is a common task for deep learning and neural networks.  The raw features coming in are the pixel values.  These are simple enough to deal with, but it is difficult to connect pixel values to determining whether an image is of a cat.  Older methods used a lot of clever filters, but the current best-of-breed algorithms simply throw a lot of linear algebra at the problem.\n",
    "\n",
    "In this miniproject, you build a series of models to classify a series of images into one of ten classes. For expediency, these images are pretty small ($32\\times32\\times3$).  This can make classification a bit tricky&mdash;human performance is only about 94%.  Each of your models will be scored by comparing its accuracy to the accuracy of a reference model that we developed.  A score of 1 indicates that your model performs as well as the reference model; not that your accuracy is 100%!\n",
    "\n",
    "You will be given both a training set and a validation set.  Ground truth values are provided for the training set.  You should train your models on this set, and then make predictions for each of the validation images.  These predictions will be submitted to the grader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## A note on scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "It **is** possible to score above 1 on these questions. This indicates that you've beaten our reference model&mdash;we compare our model's score on a test set to your score on a test set. See how high you can go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Downloading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "We will be using the `CIFAR-10` data set.  It consists of 60,000 images, each $32\\times32$ color pixels, each belonging to one of ten classes.  The following cell will download the data, in NumPy's `.npy` format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "We can load in the data like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gzip\n",
    "\n",
    "train_images = np.load(gzip.open('train_images.npy.gz', 'rb'))\n",
    "train_labels = np.load(gzip.open('train_labels.npy.gz', 'rb'))\n",
    "validation_images = np.load(gzip.open('validation_images.npy.gz', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "The images are stored as four-dimensional arrays.  The first index indicates the image number, the second and third the $x$ and $y$ positions, and the fourth index the color channel.  Each pixel color is a floating point number between 0 and 1.  This convention allows us to view the images with matplotlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f3be99d06a0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot\n",
    "matplotlib.pyplot.rcParams[\"axes.grid\"] = False  #  Remove the grid lines from the image.\n",
    "matplotlib.pyplot.imshow(train_images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "The classes have already been numbered 0-9 for us; those numbers are stored in the vector `train_labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "The human-readable names associated with this classes are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "label_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'frog'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_names[train_labels[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "So we can see that the image above is a frog.  (Now you see it!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "# Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Perceptual Delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Since we already have a number of labeled images, a simple approach would be to measure the difference between two images, and choose the label corresponding to nearby images.  To do this, we need to develop a metric to determine the distance between two images.  We'll make the simplifying (and completely wrong) assumption that this is just the average difference between the colors of the corresponding pixels in the two images.\n",
    "\n",
    "While we could just take RMS difference of the red, green, and blue pixels, let's be slightly more sophisticated and look at human vision for a metric.  After all, we're pretty good at image classifications, so there might be some useful optimization here.\n",
    "\n",
    "It turns out that modeling human perception is [extraordinarily complicated](https://en.wikipedia.org/wiki/Color_difference#CIEDE2000).  We're going to use a [simplified model](https://www.compuphase.com/cmetric.htm):\n",
    "\n",
    "$$\\Delta C \\equiv \\sqrt{2 \\Delta R^2 + 4 \\Delta G^2 + 3 \\Delta B^2 + \\bar R\\left(\\Delta R^2 - \\Delta B^2 \\right)} $$\n",
    "where $(R_1, G_1, B_1)$ and $(R_2, G_2, B_2)$ are the RGB components of the two colors and\n",
    "$$\\begin{align}\n",
    "\\Delta R &= R_1 - R_2 \\\\\n",
    "\\Delta G &= G_1 - G_2 \\\\\n",
    "\\Delta B &= B_1 - B_2 \\\\\n",
    "\\bar R &= \\textstyle\\frac{1}{2}\\left(R_1 + R_2\\right)\n",
    "\\end{align}$$\n",
    "\n",
    "This accounts for the fact that our eyes are most sensitive to green and least sensitive to red, and that perception is not constant with hue.\n",
    "\n",
    "Build a graph that takes in a series of images, as well as a base image, and returns a result containing the $\\Delta C$ value (for each pixel) between the base image and each image in the series.  \n",
    "\n",
    "(Note that the intention here in our solution is that `images` will be a stack of images, while `base` is a single image.  Since the function is decorated with `tf.function` the computations within the function will be recorded in a computation graph and the output from this function will be a tensor.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "@tf.function\n",
    "def delta_func(images, base):\n",
    "    dr = images[:, :, :, 0] - base[:, :, 0]\n",
    "    dg = images[:, :, :, 1] - base[:, :, 1]\n",
    "    db = images[:, :, :, 2] - base[:, :, 2]\n",
    "    rmean = (images[:, :, :, 0] + base[:, :, 0])/2\n",
    "\n",
    "    deltaC = (2*(dr**2) + 4*(dg**2) + 3*(db**2) + rmean*((dr**2) - (db**2)))**.5\n",
    "    return deltaC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "As stated, for two images, $I_1$ and $I_2$, we define the distance between $I_1$ and $I_2$ as the average $\\Delta C$ value over the whole image, that is:\n",
    "\n",
    "$$d(I_1, I_2) = \\frac{1}{N}\\sum_{p_j} \\Delta C(p_j)$$\n",
    "where the sum is over all pixels $p_j$, $\\Delta C(p_2)$ is the $\\Delta C$ value for the pixel $p_j$, and $N$ is the total number of pixels in each image ($N= 32\\times 32$ in our case).\n",
    "\n",
    "Using `delta_func` compute the distance between the first validation image and all of the training images.\n",
    "\n",
    "**Checkpoint:** The mean value of the distances is 1.159, and the standard deviation of the distances is 0.182."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-03 20:01:07.393025: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 614400000 exceeds 10% of free system memory.\n",
      "2022-03-03 20:01:07.760081: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 614400000 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "deltas = delta_func(train_images, validation_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = []\n",
    "\n",
    "for i in range(50000):\n",
    "    distances.append(np.mean(deltas[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "From these, find the 100 closest images from the training set to this first validation image.  (Note that `numpy.argsort` might help here.)  Submit a list of the indices of these images to the grader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([27912, 23437, 47732, 25053, 29251,  3616, 43445,  5555, 46911,\n",
       "       16269])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = np.argsort(np.array(distances))[:100]\n",
    "indices[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score: 1.0000\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "# indices = [43234]*100\n",
    "\n",
    "grader.score('tf__perceptual_delta', indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "**Extension:** What does this suggest about the proper class for this image?\n",
    "\n",
    "> **Aside:** Essentially, we've started to implement a $k$-nearest neighbors algorithm, using this perceptual distance as our metric.  If we ran the difference between all of the validation images and each of the training images, we could make a prediction from the nearest images for each.  Give it a try, if you're interested, but this miniproject is going to go in another direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Softmax model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Fully-connected model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Now, add a hidden layer to this network.  Train this network on the pixel values, and once again use it to predict the most likely class for each of the validation images.\n",
    "\n",
    "**Hints:**\n",
    "- We found that adding more layers didn't help too much.\n",
    "- Watch out for overfitting.  Dropout can help with this.\n",
    "- The reference solution achieves an accuracy of about 44% on a training set and 41% on a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [np.concatenate(np.concatenate(im)) for im in train_images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_labels[:49000]\n",
    "\n",
    "X_t = np.asarray(X_train[:49000])\n",
    "\n",
    "X_test = np.asarray(X_train[49000:])\n",
    "y_test = train_labels[49000:]\n",
    "                          \n",
    "from tensorflow.keras.utils import to_categorical as one_hot\n",
    "\n",
    "y_train_hot = one_hot(y_train)\n",
    "y_test_hot = one_hot(y_test)\n",
    "                          \n",
    "\n",
    "N_PIXELS= 32 * 32 * 3\n",
    "N_CLASSES = 10\n",
    "\n",
    "hidden_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(\n",
    "    keras.layers.Dense(\n",
    "        hidden_size,\n",
    "        activation='sigmoid',\n",
    "        use_bias=True,  # The default\n",
    "        kernel_initializer=keras.initializers.TruncatedNormal(stddev=N_PIXELS**-0.5)\n",
    "    )\n",
    ")\n",
    "\n",
    "model.add(\n",
    "    keras.layers.Dense(\n",
    "        10,\n",
    "        activation='softmax',\n",
    "        kernel_initializer=keras.initializers.TruncatedNormal(stddev=hidden_size**-0.5)\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=0.5),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "383/383 [==============================] - 2s 4ms/step - loss: 1.9731 - accuracy: 0.2835 - val_loss: 1.9252 - val_accuracy: 0.3230\n",
      "Epoch 2/30\n",
      "383/383 [==============================] - 1s 3ms/step - loss: 1.7962 - accuracy: 0.3583 - val_loss: 1.7564 - val_accuracy: 0.3900\n",
      "Epoch 3/30\n",
      "383/383 [==============================] - 1s 3ms/step - loss: 1.7234 - accuracy: 0.3835 - val_loss: 1.7241 - val_accuracy: 0.3900\n",
      "Epoch 4/30\n",
      "383/383 [==============================] - 1s 3ms/step - loss: 1.6716 - accuracy: 0.4055 - val_loss: 1.6895 - val_accuracy: 0.3940\n",
      "Epoch 5/30\n",
      "383/383 [==============================] - 1s 3ms/step - loss: 1.6258 - accuracy: 0.4193 - val_loss: 1.6519 - val_accuracy: 0.4120\n",
      "Epoch 6/30\n",
      "383/383 [==============================] - 1s 3ms/step - loss: 1.5993 - accuracy: 0.4313 - val_loss: 1.6315 - val_accuracy: 0.4180\n",
      "Epoch 7/30\n",
      "383/383 [==============================] - 1s 3ms/step - loss: 1.5732 - accuracy: 0.4397 - val_loss: 1.6308 - val_accuracy: 0.4130\n",
      "Epoch 8/30\n",
      "383/383 [==============================] - 1s 3ms/step - loss: 1.5450 - accuracy: 0.4497 - val_loss: 1.5779 - val_accuracy: 0.4410\n",
      "Epoch 9/30\n",
      "383/383 [==============================] - 1s 3ms/step - loss: 1.5237 - accuracy: 0.4578 - val_loss: 1.6402 - val_accuracy: 0.4270\n",
      "Epoch 10/30\n",
      "383/383 [==============================] - 1s 3ms/step - loss: 1.5055 - accuracy: 0.4643 - val_loss: 1.5418 - val_accuracy: 0.4600\n",
      "Epoch 11/30\n",
      "383/383 [==============================] - 1s 3ms/step - loss: 1.4865 - accuracy: 0.4711 - val_loss: 1.5582 - val_accuracy: 0.4470\n",
      "Epoch 12/30\n",
      "383/383 [==============================] - 1s 3ms/step - loss: 1.4767 - accuracy: 0.4735 - val_loss: 1.5478 - val_accuracy: 0.4500\n",
      "Epoch 13/30\n",
      "383/383 [==============================] - 1s 3ms/step - loss: 1.4654 - accuracy: 0.4785 - val_loss: 1.4772 - val_accuracy: 0.4610\n",
      "Epoch 14/30\n",
      "383/383 [==============================] - 1s 3ms/step - loss: 1.4544 - accuracy: 0.4806 - val_loss: 1.4691 - val_accuracy: 0.4740\n",
      "Epoch 15/30\n",
      "383/383 [==============================] - 1s 3ms/step - loss: 1.4404 - accuracy: 0.4871 - val_loss: 1.6022 - val_accuracy: 0.4300\n",
      "Epoch 16/30\n",
      "383/383 [==============================] - 1s 3ms/step - loss: 1.4351 - accuracy: 0.4902 - val_loss: 1.5034 - val_accuracy: 0.4620\n",
      "Epoch 17/30\n",
      "383/383 [==============================] - 1s 3ms/step - loss: 1.4221 - accuracy: 0.4933 - val_loss: 1.5017 - val_accuracy: 0.4520\n",
      "Epoch 18/30\n",
      "383/383 [==============================] - 1s 3ms/step - loss: 1.4137 - accuracy: 0.4964 - val_loss: 1.4981 - val_accuracy: 0.4510\n",
      "Epoch 19/30\n",
      "383/383 [==============================] - 1s 3ms/step - loss: 1.4021 - accuracy: 0.5021 - val_loss: 1.5575 - val_accuracy: 0.4500\n",
      "Epoch 20/30\n",
      "383/383 [==============================] - 1s 3ms/step - loss: 1.3969 - accuracy: 0.5028 - val_loss: 1.4764 - val_accuracy: 0.4690\n",
      "Epoch 21/30\n",
      "383/383 [==============================] - 1s 3ms/step - loss: 1.3865 - accuracy: 0.5086 - val_loss: 1.4894 - val_accuracy: 0.4710\n",
      "Epoch 22/30\n",
      "383/383 [==============================] - 1s 3ms/step - loss: 1.3818 - accuracy: 0.5103 - val_loss: 1.4915 - val_accuracy: 0.4540\n",
      "Epoch 23/30\n",
      "383/383 [==============================] - 1s 3ms/step - loss: 1.3710 - accuracy: 0.5121 - val_loss: 1.4912 - val_accuracy: 0.4640\n",
      "Epoch 24/30\n",
      "383/383 [==============================] - 1s 3ms/step - loss: 1.3640 - accuracy: 0.5172 - val_loss: 1.4993 - val_accuracy: 0.4760\n",
      "Epoch 25/30\n",
      "383/383 [==============================] - 1s 3ms/step - loss: 1.3595 - accuracy: 0.5162 - val_loss: 1.4432 - val_accuracy: 0.4790\n",
      "Epoch 26/30\n",
      "383/383 [==============================] - 1s 3ms/step - loss: 1.3493 - accuracy: 0.5219 - val_loss: 1.4893 - val_accuracy: 0.4610\n",
      "Epoch 27/30\n",
      "383/383 [==============================] - 1s 3ms/step - loss: 1.3502 - accuracy: 0.5190 - val_loss: 1.4849 - val_accuracy: 0.4650\n",
      "Epoch 28/30\n",
      "383/383 [==============================] - 1s 3ms/step - loss: 1.3392 - accuracy: 0.5253 - val_loss: 1.4329 - val_accuracy: 0.4750\n",
      "Epoch 29/30\n",
      "383/383 [==============================] - 1s 3ms/step - loss: 1.3306 - accuracy: 0.5273 - val_loss: 1.5763 - val_accuracy: 0.4610\n",
      "Epoch 30/30\n",
      "383/383 [==============================] - 1s 3ms/step - loss: 1.3206 - accuracy: 0.5314 - val_loss: 1.4905 - val_accuracy: 0.4760\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_t, y_train_hot,\n",
    "                    epochs=30,\n",
    "                    batch_size=128,\n",
    "                    validation_data=(X_test,y_test_hot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = [np.concatenate(np.concatenate(im)) for im in validation_images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_classes = list(np.argmax(model.predict(np.asarray(val)),axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score: 1.2768\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "# predicted_classes = [0]*len(validation_images)\n",
    "\n",
    "grader.score('tf__fully_connected', predicted_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Convolutional model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "*Copyright &copy; 2022 Pragmatic Institute. This content is licensed solely for personal use. Redistribution or publication of this material in whole is strictly prohibited.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
